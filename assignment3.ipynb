{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b75ae29a",
   "metadata": {},
   "source": [
    "### Read in and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "902ade84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: sap_press_clean.csv\n",
      "Final rows: 1410\n",
      "Classes kept: 7\n",
      "Removed rare labels (count < {MIN_SAMPLES_PER_CLASS}):\n",
      "  - Bill Payment and More (count=1)\n",
      "\n",
      "Label distribution:\n",
      "label\n",
      "Partnership      468\n",
      "Award            328\n",
      "Story            270\n",
      "Financials       157\n",
      "Solution          81\n",
      "Merger/Invest     65\n",
      "People            41\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "INPUT_PATH = \"sap_press.csv\"\n",
    "OUTPUT_PATH = \"sap_press_clean.csv\"\n",
    "TEXT_COL = \"headline\"\n",
    "LABEL_COL = \"label\"\n",
    "MIN_SAMPLES_PER_CLASS = 2\n",
    "\n",
    "def normalize_label(x):\n",
    "    if pd.isna(x): return x\n",
    "    s = str(x).strip()\n",
    "    if len(s) >= 2 and ((s[0] == s[-1] == '\"') or (s[0] == s[-1] == \"'\")):\n",
    "        s = s[1:-1].strip()\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip(\" '\\\"\\t\\n\\r\")\n",
    "\n",
    "def read_csv_robust(p):\n",
    "    for enc in [\"utf-8\", \"utf-8-sig\", \"latin-1\"]:\n",
    "        try:\n",
    "            return pd.read_csv(p, encoding=enc)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return pd.read_csv(p)\n",
    "\n",
    "# 1) read\n",
    "df = read_csv_robust(INPUT_PATH)\n",
    "df.columns = [c.strip().lower() for c in df.columns]\n",
    "assert TEXT_COL in df.columns and LABEL_COL in df.columns, df.columns\n",
    "\n",
    "# 2) select & basic clean\n",
    "keep_cols = [TEXT_COL, LABEL_COL] if \"id\" not in df.columns else [\"id\", TEXT_COL, LABEL_COL]\n",
    "df = df[keep_cols].copy()\n",
    "df[TEXT_COL] = df[TEXT_COL].astype(str).str.strip()\n",
    "df = df[df[TEXT_COL].str.len() > 0]\n",
    "df[LABEL_COL] = df[LABEL_COL].apply(normalize_label)\n",
    "df = df[df[LABEL_COL].notna() & (df[LABEL_COL].astype(str).str.len() > 0)]\n",
    "\n",
    "# 3) deduplicate pairs\n",
    "df = df.drop_duplicates(subset=[TEXT_COL, LABEL_COL])\n",
    "\n",
    "# 4) remove rare classes\n",
    "vc = df[LABEL_COL].value_counts()\n",
    "rare_labels = vc[vc < MIN_SAMPLES_PER_CLASS].index.tolist()\n",
    "removed_before = len(df)\n",
    "if rare_labels:\n",
    "    df = df[~df[LABEL_COL].isin(rare_labels)].copy()\n",
    "removed_rare = removed_before - len(df)\n",
    "\n",
    "# 5) save + report\n",
    "df.to_csv(OUTPUT_PATH, index=False)\n",
    "\n",
    "print(\"Saved:\", OUTPUT_PATH)\n",
    "print(f\"Final rows: {len(df)}\")\n",
    "print(f\"Classes kept: {df[LABEL_COL].nunique()}\")\n",
    "if rare_labels:\n",
    "    print(\"Removed rare labels (count < {MIN_SAMPLES_PER_CLASS}):\")\n",
    "    for lbl in rare_labels:\n",
    "        print(f\"  - {lbl} (count={int(vc[lbl])})\")\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df[LABEL_COL].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8125fd",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8597d537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cleaned dataset: sap_press_clean.csv\n",
      "=== Label Distribution ===\n",
      "           label  count  percent\n",
      "0    Partnership    468    33.19\n",
      "1          Award    328    23.26\n",
      "2          Story    270    19.15\n",
      "3     Financials    157    11.13\n",
      "4       Solution     81     5.74\n",
      "5  Merger/Invest     65     4.61\n",
      "6         People     41     2.91\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x645 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "CLEAN_PATH = \"sap_press_clean.csv\"\n",
    "TEXT_COL = \"headline\"\n",
    "LABEL_COL = \"label\"\n",
    "\n",
    "\n",
    "def load_dataset():\n",
    "    p = Path(CLEAN_PATH)\n",
    "    df = pd.read_csv(CLEAN_PATH)\n",
    "    print(f\"Loaded cleaned dataset: {CLEAN_PATH}\")\n",
    "    return df\n",
    "\n",
    "df = load_dataset()\n",
    "\n",
    "cols = [c.lower() for c in df.columns]\n",
    "\n",
    "df.columns = [c.lower() for c in df.columns]\n",
    "TEXT_COL = TEXT_COL.lower()\n",
    "LABEL_COL = LABEL_COL.lower()\n",
    "\n",
    "counts = df[LABEL_COL].value_counts(dropna=False)\n",
    "percent = (counts / counts.sum() * 100).round(2)\n",
    "dist = pd.DataFrame({\"count\": counts, \"percent\": percent}).reset_index(names=LABEL_COL)\n",
    "\n",
    "dist = dist.sort_values(\"count\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"=== Label Distribution ===\")\n",
    "print(dist)\n",
    "\n",
    "plt.figure(figsize=(8, 4 + 0.35 * len(dist)))\n",
    "plt.barh(dist[LABEL_COL], dist[\"count\"])\n",
    "plt.xlabel(\"Number of texts\")\n",
    "plt.ylabel(\"Label\")\n",
    "plt.title(\"Texts per Label\")\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "for i, (lbl, cnt) in enumerate(zip(dist[LABEL_COL], dist[\"count\"])):\n",
    "    plt.text(cnt, i, f\"  {cnt}\", va=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997a3c52",
   "metadata": {},
   "source": [
    "### Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16d89de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Test Metrics ===\n",
      "{'accuracy': 0.4695035460992908, 'macro_f1': np.float64(0.4175687933662365), 'weighted_f1': np.float64(0.4884229281689336)}\n",
      "\n",
      "=== Classification Report ===\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Award       0.72      0.66      0.69       328\n",
      "   Financials       0.67      0.70      0.69       157\n",
      "Merger/Invest       0.66      0.42      0.51        65\n",
      "  Partnership       0.72      0.47      0.57       468\n",
      "       People       0.17      0.10      0.12        41\n",
      "     Solution       0.13      0.77      0.23        81\n",
      "        Story       0.21      0.09      0.12       270\n",
      "\n",
      "     accuracy                           0.47      1410\n",
      "    macro avg       0.47      0.46      0.42      1410\n",
      " weighted avg       0.56      0.47      0.49      1410\n",
      "\n",
      "\n",
      "Saved confusion matrix to: zero_shot_eval\\confusion_matrix_zero_shot.csv\n",
      "Saved per-example predictions to: zero_shot_eval\\zero_shot_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"; os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "CLEAN_PATH = \"sap_press_clean.csv\"\n",
    "TEXT_COL, LABEL_COL = \"headline\", \"label\"\n",
    "MODEL_ZS = \"roberta-large-mnli\"\n",
    "BATCH = 16\n",
    "OUT_DIR = Path(\"zero_shot_eval\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Load data & canonical label maps (ordered) ---\n",
    "df = pd.read_csv(CLEAN_PATH)\n",
    "df.columns = [c.strip().lower() for c in df.columns]\n",
    "df = df[[TEXT_COL, LABEL_COL]].rename(columns={TEXT_COL:\"text\", LABEL_COL:\"label\"})\n",
    "\n",
    "labels = sorted(df[\"label\"].dropna().unique().tolist())\n",
    "num_labels = len(labels)\n",
    "label2id = {lbl: i for i, lbl in enumerate(labels)}\n",
    "id2label = {i: lbl for i, lbl in enumerate(labels)}\n",
    "index2name = [id2label[i] for i in range(num_labels)]\n",
    "\n",
    "# --- Zero-shot classifier ---\n",
    "clf = pipeline(\"zero-shot-classification\", model=MODEL_ZS, device_map=\"auto\")\n",
    "\n",
    "def predict_zero_shot_batch(texts, candidate_labels=labels, multi_label=False):\n",
    "    out = clf(texts, candidate_labels=candidate_labels, multi_label=multi_label)\n",
    "    if isinstance(out, dict):\n",
    "        out = [out]\n",
    "    return [o[\"labels\"][0] for o in out]\n",
    "\n",
    "# --- Run predictions in batches ---\n",
    "pred_labels = []\n",
    "texts = df[\"text\"].tolist()\n",
    "for i in range(0, len(texts), BATCH):\n",
    "    chunk = texts[i:i+BATCH]\n",
    "    pred_labels.extend(predict_zero_shot_batch(chunk))\n",
    "\n",
    "# --- Encode y_true / y_pred to ids (stable ordering) ---\n",
    "y_true = np.array([label2id[lbl] for lbl in df[\"label\"].tolist()])\n",
    "y_pred = np.array([label2id[pl] for pl in pred_labels])\n",
    "\n",
    "# --- Metrics  ---\n",
    "print(\"\\n=== Test Metrics ===\")\n",
    "print({\n",
    "    \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "    \"macro_f1\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "    \"weighted_f1\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "})\n",
    "\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(\n",
    "    y_true, y_pred,\n",
    "    labels=list(range(num_labels)),\n",
    "    target_names=index2name,\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "# --- Confusion matrix  ---\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(num_labels)))\n",
    "cm_df = pd.DataFrame(cm, index=index2name, columns=index2name)\n",
    "cm_path = OUT_DIR / \"confusion_matrix_zero_shot.csv\"\n",
    "cm_df.to_csv(cm_path, index=True)\n",
    "print(f\"\\nSaved confusion matrix to: {cm_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3356efb4",
   "metadata": {},
   "source": [
    "### Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574b4e9f",
   "metadata": {},
   "source": [
    "#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c4edd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24835c691b14358b608aabf6595fed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa93cb0d78c4adf97b74b27f871ad49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/141 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e9f9ba704cf4688ae92b2095f18d9dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/141 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large-mnli and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 1024]) in the checkpoint and torch.Size([7, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_19268\\1569867505.py:123: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='423' max='423' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [423/423 20:57, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.108700</td>\n",
       "      <td>0.886694</td>\n",
       "      <td>0.687943</td>\n",
       "      <td>0.487716</td>\n",
       "      <td>0.598011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.525200</td>\n",
       "      <td>0.507026</td>\n",
       "      <td>0.822695</td>\n",
       "      <td>0.704948</td>\n",
       "      <td>0.797543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.327100</td>\n",
       "      <td>0.586818</td>\n",
       "      <td>0.822695</td>\n",
       "      <td>0.727460</td>\n",
       "      <td>0.793282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "c:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best checkpoint: roberta_large_mnli_topic_detection\\checkpoint-423\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: {'eval_loss': 0.5868181586265564, 'eval_accuracy': 0.8226950354609929, 'eval_macro_f1': 0.7274596405726469, 'eval_weighted_f1': 0.7932815658284061, 'eval_runtime': 7.9356, 'eval_samples_per_second': 17.768, 'eval_steps_per_second': 1.134, 'epoch': 3.0}\n",
      "\n",
      "=== Test Metrics ===\n",
      "{'accuracy': 0.8085106382978723, 'macro_f1': np.float64(0.7106595206890095), 'weighted_f1': np.float64(0.7833575239582462)}\n",
      "\n",
      "=== Classification Report ===\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Award       0.97      0.91      0.94        33\n",
      "   Financials       1.00      1.00      1.00        15\n",
      "Merger/Invest       0.71      0.71      0.71         7\n",
      "  Partnership       0.73      0.96      0.83        47\n",
      "       People       1.00      0.75      0.86         4\n",
      "     Solution       0.00      0.00      0.00         8\n",
      "        Story       0.70      0.59      0.64        27\n",
      "\n",
      "     accuracy                           0.81       141\n",
      "    macro avg       0.73      0.70      0.71       141\n",
      " weighted avg       0.77      0.81      0.78       141\n",
      "\n",
      "\n",
      "Saved confusion matrix to: roberta_large_mnli_topic_detection\\confusion_matrix_full.csv\n",
      "\n",
      "Examples:\n",
      "{'text': 'Company announces strategic partnership to expand cloud footprint', 'pred': 'Partnership', 'conf': 0.8794620633125305}\n",
      "{'text': 'CFO reports record quarterly revenue growth', 'pred': 'Financials', 'conf': 0.9804489016532898}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import random, torch\n",
    "from inspect import signature\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# ------------------ Config ------------------\n",
    "CLEAN_PATH = \"sap_press_clean.csv\"\n",
    "TEXT_COL, LABEL_COL = \"headline\", \"label\"\n",
    "MODEL_NAME = \"roberta-large-mnli\"\n",
    "MAX_LEN = 128\n",
    "EPOCHS = 3\n",
    "LR = 1.5e-5\n",
    "BATCH_TRAIN = 8\n",
    "BATCH_EVAL = 16\n",
    "SEED = 4213\n",
    "OUTPUT_DIR = \"roberta_large_mnli_topic_detection\"\n",
    "\n",
    "GRADIENT_CHECKPOINTING = True\n",
    "WARMUP_RATIO = 0.1\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# ------------------ Repro -------------------\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "set_seed()\n",
    "\n",
    "# ------------------ Data --------------------\n",
    "df = pd.read_csv(CLEAN_PATH)\n",
    "df.columns = [c.strip().lower() for c in df.columns]\n",
    "assert TEXT_COL in df.columns and LABEL_COL in df.columns, df.columns\n",
    "df = df[[TEXT_COL, LABEL_COL]].rename(columns={TEXT_COL: \"text\", LABEL_COL: \"label\"})\n",
    "\n",
    "labels = sorted(df[\"label\"].unique().tolist())\n",
    "label2id = {lbl: i for i, lbl in enumerate(labels)}\n",
    "id2label = {i: lbl for i, lbl in enumerate(labels)}\n",
    "num_labels = len(labels)\n",
    "df[\"label_id\"] = df[\"label\"].map(label2id)\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=SEED, stratify=df[\"label_id\"])\n",
    "val_df,   test_df = train_test_split(temp_df, test_size=0.5, random_state=SEED, stratify=temp_df[\"label_id\"])\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df[[\"text\", \"label_id\"]].reset_index(drop=True)),\n",
    "    \"validation\": Dataset.from_pandas(val_df[[\"text\", \"label_id\"]].reset_index(drop=True)),\n",
    "    \"test\": Dataset.from_pandas(test_df[[\"text\", \"label_id\"]].reset_index(drop=True)),\n",
    "})\n",
    "\n",
    "ds = ds.rename_column(\"label_id\", \"labels\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "def tok_fn(ex): return tokenizer(ex[\"text\"], truncation=True, max_length=MAX_LEN)\n",
    "tok = ds.map(tok_fn, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=num_labels, id2label=id2label, label2id=label2id, ignore_mismatched_sizes=True\n",
    ")\n",
    "if GRADIENT_CHECKPOINTING:\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, y_true = eval_pred\n",
    "    y_pred = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": acc_metric.compute(predictions=y_pred, references=y_true)[\"accuracy\"],\n",
    "        \"macro_f1\": f1_metric.compute(predictions=y_pred, references=y_true, average=\"macro\")[\"f1\"],\n",
    "        \"weighted_f1\": f1_metric.compute(predictions=y_pred, references=y_true, average=\"weighted\")[\"f1\"],\n",
    "    }\n",
    "\n",
    "# TrainingArguments\n",
    "ta_kwargs = dict(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_TRAIN,\n",
    "    per_device_eval_batch_size=BATCH_EVAL,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    logging_steps=50,\n",
    "    seed=SEED,\n",
    "    report_to=[],\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    gradient_checkpointing=GRADIENT_CHECKPOINTING,\n",
    ")\n",
    "sig = signature(TrainingArguments.__init__)\n",
    "if \"evaluation_strategy\" in sig.parameters:\n",
    "    ta_kwargs[\"evaluation_strategy\"] = \"epoch\"\n",
    "else:\n",
    "    ta_kwargs[\"eval_strategy\"] = \"epoch\"\n",
    "\n",
    "args = TrainingArguments(**ta_kwargs)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tok[\"train\"],\n",
    "    eval_dataset=tok[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")\n",
    "\n",
    "# ------------------ Train -------------------\n",
    "trainer.train()\n",
    "print(\"Best checkpoint:\", trainer.state.best_model_checkpoint)\n",
    "print(\"Validation:\", trainer.evaluate())\n",
    "\n",
    "# ------------------ Test --------------------\n",
    "pred = trainer.predict(tok[\"test\"])\n",
    "y_true = pred.label_ids\n",
    "y_pred = np.argmax(pred.predictions, axis=-1)\n",
    "\n",
    "print(\"\\n=== Test Metrics ===\")\n",
    "print({\n",
    "    \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "    \"macro_f1\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "    \"weighted_f1\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "})\n",
    "\n",
    "index2name = [id2label[i] for i in range(num_labels)]\n",
    "\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(classification_report(\n",
    "    y_true, y_pred,\n",
    "    labels=list(range(num_labels)),\n",
    "    target_names=index2name,\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(num_labels)))\n",
    "cm_df = pd.DataFrame(cm, index=index2name, columns=index2name)\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "cm_path = Path(OUTPUT_DIR) / \"confusion_matrix_full.csv\"\n",
    "cm_df.to_csv(cm_path, index=True)\n",
    "print(f\"\\nSaved confusion matrix to: {cm_path}\")\n",
    "\n",
    "# ------------------ Save model ----------------\n",
    "final_dir = Path(OUTPUT_DIR) / \"final_full\"\n",
    "final_dir.mkdir(parents=True, exist_ok=True)\n",
    "trainer.save_model(final_dir.as_posix())\n",
    "tokenizer.save_pretrained(final_dir.as_posix())\n",
    "\n",
    "# ------------------ Quick inference helper ----\n",
    "def predict_topics(texts):\n",
    "    enc = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_LEN)\n",
    "    enc = {k: v.to(model.device) for k, v in enc.items()}\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        probs = torch.softmax(model(**enc).logits, dim=-1).cpu().numpy()\n",
    "    pred_ids = probs.argmax(axis=-1)\n",
    "    return [{\"text\": t, \"pred\": id2label[i], \"conf\": float(probs[j, i])} for j,(t,i) in enumerate(zip(texts, pred_ids))]\n",
    "\n",
    "print(\"\\nExamples:\")\n",
    "for r in predict_topics([\n",
    "    \"Company announces strategic partnership to expand cloud footprint\",\n",
    "    \"CFO reports record quarterly revenue growth\",\n",
    "]):\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccb2154",
   "metadata": {},
   "source": [
    "#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece0eb42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce1233e8aaa405dbadb75bcbd65573e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1128 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4690af926d67452d826fa19815bb729c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/141 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cfa7b85075549df96ade5a28e18da9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/141 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large-mnli and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 1024]) in the checkpoint and torch.Size([7, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_19268\\3464191232.py:114: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='705' max='705' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [705/705 13:17, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Macro F1</th>\n",
       "      <th>Weighted F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.468100</td>\n",
       "      <td>1.109648</td>\n",
       "      <td>0.645390</td>\n",
       "      <td>0.384614</td>\n",
       "      <td>0.576739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.853900</td>\n",
       "      <td>1.066241</td>\n",
       "      <td>0.673759</td>\n",
       "      <td>0.458415</td>\n",
       "      <td>0.620108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.727500</td>\n",
       "      <td>0.784476</td>\n",
       "      <td>0.758865</td>\n",
       "      <td>0.564154</td>\n",
       "      <td>0.721060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.680300</td>\n",
       "      <td>0.985273</td>\n",
       "      <td>0.709220</td>\n",
       "      <td>0.521349</td>\n",
       "      <td>0.662798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.503500</td>\n",
       "      <td>0.758396</td>\n",
       "      <td>0.744681</td>\n",
       "      <td>0.587915</td>\n",
       "      <td>0.715360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best checkpoint: roberta_large_mnli_lora\\checkpoint-705\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: {'eval_loss': 0.7583962082862854, 'eval_accuracy': 0.7446808510638298, 'eval_macro_f1': 0.587914846214367, 'eval_weighted_f1': 0.7153596823399957, 'eval_runtime': 8.1776, 'eval_samples_per_second': 17.242, 'eval_steps_per_second': 1.101, 'epoch': 5.0}\n",
      "\n",
      "=== Test Metrics (LoRA on roberta-large-mnli) ===\n",
      "{'accuracy': 0.7659574468085106, 'macro_f1': np.float64(0.6321514909506902), 'weighted_f1': np.float64(0.7441102434950675)}\n",
      "\n",
      "=== Classification Report (LoRA) ===\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        Award       1.00      0.85      0.92        33\n",
      "   Financials       0.94      1.00      0.97        15\n",
      "Merger/Invest       0.50      0.57      0.53         7\n",
      "  Partnership       0.66      0.91      0.77        47\n",
      "       People       0.67      0.50      0.57         4\n",
      "     Solution       0.00      0.00      0.00         8\n",
      "        Story       0.76      0.59      0.67        27\n",
      "\n",
      "     accuracy                           0.77       141\n",
      "    macro avg       0.65      0.63      0.63       141\n",
      " weighted avg       0.74      0.77      0.74       141\n",
      "\n",
      "\n",
      "Saved confusion matrix to: roberta_large_mnli_lora\\confusion_matrix_lora.csv\n",
      "\n",
      "Examples (LoRA on roberta-large-mnli):\n",
      "{'text': 'Company announces strategic partnership to expand cloud footprint', 'pred': 'Partnership', 'conf': 0.9641020894050598}\n",
      "{'text': 'CFO reports record quarterly revenue growth', 'pred': 'Financials', 'conf': 0.9961915016174316}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"; os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
    "\n",
    "import random, numpy as np, pandas as pd, torch\n",
    "from pathlib import Path\n",
    "from inspect import signature\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
    "                          DataCollatorWithPadding, TrainingArguments, Trainer, EarlyStoppingCallback)\n",
    "import evaluate\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# -------- Config --------\n",
    "CLEAN_PATH = \"sap_press_clean.csv\"\n",
    "TEXT_COL, LABEL_COL = \"headline\", \"label\"\n",
    "MODEL_NAME = \"roberta-large-mnli\"\n",
    "MAX_LEN = 128\n",
    "EPOCHS = 5\n",
    "LR = 2e-4\n",
    "BATCH_TRAIN = 8\n",
    "BATCH_EVAL  = 16\n",
    "SEED = 4213\n",
    "OUTPUT_DIR = \"roberta_large_mnli_lora\"\n",
    "\n",
    "# LoRA params\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.1\n",
    "TARGET_MODULES = [\"query\", \"value\"]\n",
    "\n",
    "def set_seed(s=SEED):\n",
    "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "set_seed()\n",
    "\n",
    "# -------- Data --------\n",
    "df = pd.read_csv(CLEAN_PATH)\n",
    "df.columns = [c.strip().lower() for c in df.columns]\n",
    "assert TEXT_COL in df.columns and LABEL_COL in df.columns, df.columns\n",
    "df = df[[TEXT_COL, LABEL_COL]].rename(columns={TEXT_COL:\"text\", LABEL_COL:\"label\"})\n",
    "\n",
    "labels = sorted(df[\"label\"].unique().tolist())\n",
    "label2id = {lbl:i for i,lbl in enumerate(labels)}\n",
    "id2label = {i:lbl for i,lbl in enumerate(labels)}\n",
    "num_labels = len(labels)\n",
    "df[\"label_id\"] = df[\"label\"].map(label2id)\n",
    "\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=SEED, stratify=df[\"label_id\"])\n",
    "val_df,   test_df = train_test_split(temp_df, test_size=0.5, random_state=SEED, stratify=temp_df[\"label_id\"])\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df[[\"text\",\"label_id\"]].reset_index(drop=True)),\n",
    "    \"validation\": Dataset.from_pandas(val_df[[\"text\",\"label_id\"]].reset_index(drop=True)),\n",
    "    \"test\": Dataset.from_pandas(test_df[[\"text\",\"label_id\"]].reset_index(drop=True)),\n",
    "}).rename_column(\"label_id\",\"labels\")\n",
    "\n",
    "# -------- Tokenizer --------\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "def tok_fn(ex): return tokenizer(ex[\"text\"], truncation=True, max_length=MAX_LEN)\n",
    "tok = ds.map(tok_fn, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# -------- Base model + LoRA --------\n",
    "base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels, id2label=id2label, label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "lora_cfg = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS, r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=TARGET_MODULES, bias=\"none\"\n",
    ")\n",
    "model = get_peft_model(base, lora_cfg)\n",
    "\n",
    "collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "acc_metric, f1_metric = evaluate.load(\"accuracy\"), evaluate.load(\"f1\")\n",
    "def compute_metrics(p):\n",
    "    logits, y_true = p\n",
    "    y_pred = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": acc_metric.compute(predictions=y_pred, references=y_true)[\"accuracy\"],\n",
    "        \"macro_f1\": f1_metric.compute(predictions=y_pred, references=y_true, average=\"macro\")[\"f1\"],\n",
    "        \"weighted_f1\": f1_metric.compute(predictions=y_pred, references=y_true, average=\"weighted\")[\"f1\"],\n",
    "    }\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "ta_kwargs = dict(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_TRAIN,\n",
    "    per_device_eval_batch_size=BATCH_EVAL,\n",
    "    learning_rate=LR,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=50,\n",
    "    seed=SEED,\n",
    "    report_to=[],\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "if \"evaluation_strategy\" in signature(TrainingArguments.__init__).parameters:\n",
    "    ta_kwargs[\"evaluation_strategy\"] = \"epoch\"\n",
    "else:\n",
    "    ta_kwargs[\"eval_strategy\"] = \"epoch\"\n",
    "args = TrainingArguments(**ta_kwargs)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=args,\n",
    "    train_dataset=tok[\"train\"], eval_dataset=tok[\"validation\"],\n",
    "    tokenizer=tokenizer, data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")\n",
    "\n",
    "# -------- Train --------\n",
    "trainer.train()\n",
    "print(\"Best checkpoint:\", trainer.state.best_model_checkpoint)\n",
    "print(\"Validation:\", trainer.evaluate())\n",
    "\n",
    "# -------- Test --------\n",
    "pred = trainer.predict(tok[\"test\"])\n",
    "y_true, y_pred = pred.label_ids, pred.predictions.argmax(axis=-1)\n",
    "\n",
    "print(\"\\n=== Test Metrics (LoRA on roberta-large-mnli) ===\")\n",
    "print({\n",
    "    \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "    \"macro_f1\": f1_score(y_true, y_pred, average=\"macro\"),\n",
    "    \"weighted_f1\": f1_score(y_true, y_pred, average=\"weighted\"),\n",
    "})\n",
    "\n",
    "names = [id2label[i] for i in range(num_labels)]\n",
    "print(\"\\n=== Classification Report (LoRA) ===\")\n",
    "print(classification_report(y_true, y_pred, labels=list(range(num_labels)), target_names=names, zero_division=0))\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(num_labels)))\n",
    "cm_path = Path(OUTPUT_DIR) / \"confusion_matrix_lora.csv\"\n",
    "pd.DataFrame(cm, index=names, columns=names).to_csv(cm_path, index=True)\n",
    "print(f\"\\nSaved confusion matrix to: {cm_path}\")\n",
    "\n",
    "# -------- Save adapters --------\n",
    "final_dir = Path(OUTPUT_DIR) / \"final_lora\"\n",
    "final_dir.mkdir(parents=True, exist_ok=True)\n",
    "trainer.save_model(final_dir.as_posix())\n",
    "tokenizer.save_pretrained(final_dir.as_posix())\n",
    "\n",
    "# -------- Inference helper (with adapters) --------\n",
    "def predict_topics(texts):\n",
    "    enc = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_LEN)\n",
    "    enc = {k: v.to(model.device) for k,v in enc.items()}\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        probs = torch.softmax(model(**enc).logits, dim=-1).cpu().numpy()\n",
    "    pred_ids = probs.argmax(axis=-1)\n",
    "    return [{\"text\": t, \"pred\": id2label[i], \"conf\": float(probs[j, i])} for j,(t,i) in enumerate(zip(texts, pred_ids))]\n",
    "\n",
    "print(\"\\nExamples (LoRA on roberta-large-mnli):\")\n",
    "for r in predict_topics([\n",
    "    \"Company announces strategic partnership to expand cloud footprint\",\n",
    "    \"CFO reports record quarterly revenue growth\",\n",
    "]):\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74034398",
   "metadata": {},
   "source": [
    "### Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad15f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/3] Zero-shot predictions (roberta-large-mnli)…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: error_analysis\\errors_zero_shot.csv  (errors: 748/1410)\n",
      "Zero-shot metrics: {'acc': 0.4695035460992908, 'macro_f1': np.float64(0.4175687933662365), 'weighted_f1': np.float64(0.4884229281689336)}\n",
      "\n",
      "[2/3] Full FT predictions (roberta-large-mnli fine-tuned)…\n",
      "Saved: error_analysis\\errors_full.csv  (errors: 180/1410)\n",
      "Full-FT metrics: {'acc': 0.8723404255319149, 'macro_f1': np.float64(0.7791628150462702), 'weighted_f1': np.float64(0.8492065439604237)}\n",
      "\n",
      "[3/3] LoRA FT predictions (roberta-large-mnli + adapters)…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large-mnli and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
      "- classifier.out_proj.weight: found shape torch.Size([3, 1024]) in the checkpoint and torch.Size([7, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: error_analysis\\errors_lora.csv  (errors: 260/1410)\n",
      "LoRA-FT metrics: {'acc': 0.8156028368794326, 'macro_f1': np.float64(0.6975672712130391), 'weighted_f1': np.float64(0.7900388596856006)}\n",
      "Saved: error_analysis\\all_preds.csv\n",
      "Saved: error_analysis\\disagreements.csv\n",
      "Saved: error_analysis\\top_confusions_zero_shot.csv\n",
      "Saved: error_analysis\\hardest_zero_shot.csv\n",
      "Saved: error_analysis\\top_confusions_full.csv\n",
      "Saved: error_analysis\\hardest_full.csv\n",
      "Saved: error_analysis\\top_confusions_lora.csv\n",
      "Saved: error_analysis\\hardest_lora.csv\n",
      "\n",
      "Done. Check the 'error_analysis/' folder for CSV outputs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"; os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from peft import PeftModel\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "CLEAN_PATH = \"sap_press_clean.csv\"\n",
    "TEXT_COL, LABEL_COL = \"headline\", \"label\"\n",
    "\n",
    "FULL_DIR = Path(\"roberta_large_mnli_topic_detection/final_full\")\n",
    "LORA_DIR = Path(\"roberta_large_mnli_lora/final_lora\")\n",
    "BASELINE_NAME = \"roberta-large-mnli\" \n",
    "MAX_LEN = 128\n",
    "BATCH = 16\n",
    "\n",
    "OUT_DIR = Path(\"error_analysis\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "# ---------------- Data & label maps ----------------\n",
    "df = pd.read_csv(CLEAN_PATH)\n",
    "df.columns = [c.strip().lower() for c in df.columns]\n",
    "df = df[[TEXT_COL, LABEL_COL]].rename(columns={TEXT_COL: \"text\", LABEL_COL: \"label\"}).reset_index(drop=True)\n",
    "\n",
    "labels = sorted(df[\"label\"].dropna().unique().tolist())\n",
    "num_labels = len(labels)\n",
    "label2id = {lbl: i for i, lbl in enumerate(labels)}\n",
    "id2label = {i: lbl for i, lbl in enumerate(labels)}\n",
    "\n",
    "y_true = np.array([label2id[l] for l in df[\"label\"]])\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def save_errors(df_all, model_tag):\n",
    "    errs = df_all[df_all[\"is_correct\"] == False].copy()\n",
    "    p = OUT_DIR / f\"errors_{model_tag}.csv\"\n",
    "    errs.to_csv(p, index=False)\n",
    "    print(f\"Saved: {p}  (errors: {len(errs)}/{len(df_all)})\")\n",
    "\n",
    "def top_confusions(y_true, y_pred, id2label, k=10, fname=\"top_confusions.csv\"):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(id2label))))\n",
    "    pairs = []\n",
    "    for i in range(len(id2label)):\n",
    "        for j in range(len(id2label)):\n",
    "            if i != j and cm[i, j] > 0:\n",
    "                pairs.append((id2label[i], id2label[j], int(cm[i, j])))\n",
    "    pairs = sorted(pairs, key=lambda x: x[2], reverse=True)[:k]\n",
    "    df_pairs = pd.DataFrame(pairs, columns=[\"true\", \"pred\", \"count\"])\n",
    "    path = OUT_DIR / fname\n",
    "    df_pairs.to_csv(path, index=False)\n",
    "    print(f\"Saved: {path}\")\n",
    "    return df_pairs\n",
    "\n",
    "def hardest_examples(df_src, y_true, y_pred, conf, id2label, per_class=5, fname=\"hardest_examples.csv\"):\n",
    "    rows = []\n",
    "    for cid, cname in id2label.items():\n",
    "        idxs = np.where(y_true == cid)[0]\n",
    "        if len(idxs) == 0: \n",
    "            continue\n",
    "        wrong = [i for i in idxs if y_pred[i] != y_true[i]]\n",
    "        wrong_sorted = sorted(wrong, key=lambda i: conf[i], reverse=True)[:per_class]\n",
    "        for i in wrong_sorted:\n",
    "            rows.append({\"class\": cname, \"type\": \"wrong_high_conf\", \"text\": df_src[\"text\"].iloc[i],\n",
    "                         \"true\": id2label[y_true[i]], \"pred\": id2label[y_pred[i]], \"conf\": float(conf[i])})\n",
    "        correct = [i for i in idxs if y_pred[i] == y_true[i]]\n",
    "        correct_sorted = sorted(correct, key=lambda i: conf[i])[:per_class]\n",
    "        for i in correct_sorted:\n",
    "            rows.append({\"class\": cname, \"type\": \"correct_low_conf\", \"text\": df_src[\"text\"].iloc[i],\n",
    "                         \"true\": id2label[y_true[i]], \"pred\": id2label[y_pred[i]], \"conf\": float(conf[i])})\n",
    "    df_hard = pd.DataFrame(rows)\n",
    "    path = OUT_DIR / fname\n",
    "    df_hard.to_csv(path, index=False)\n",
    "    print(f\"Saved: {path}\")\n",
    "    return df_hard\n",
    "\n",
    "# ================= 1) Zero-shot (baseline) =================\n",
    "print(\"\\n[1/3] Zero-shot predictions (roberta-large-mnli)…\")\n",
    "zs = pipeline(\"zero-shot-classification\", model=BASELINE_NAME, device=device)\n",
    "# batch predict\n",
    "pred_labels = []\n",
    "pred_scores = []\n",
    "texts = df[\"text\"].tolist()\n",
    "for i in range(0, len(texts), BATCH):\n",
    "    chunk = texts[i:i+BATCH]\n",
    "    out = zs(chunk, candidate_labels=labels, multi_label=False)\n",
    "    if isinstance(out, dict): out = [out]\n",
    "    for o in out:\n",
    "        pred_labels.append(o[\"labels\"][0])\n",
    "        pred_scores.append(float(o[\"scores\"][0]))\n",
    "y_pred_zs = np.array([label2id[p] for p in pred_labels])\n",
    "conf_zs = np.array(pred_scores)\n",
    "\n",
    "df_zs = pd.DataFrame({\n",
    "    \"text\": df[\"text\"],\n",
    "    \"true_label\": df[\"label\"],\n",
    "    \"pred_label\": [id2label[i] for i in y_pred_zs],\n",
    "    \"confidence\": conf_zs,\n",
    "})\n",
    "df_zs[\"is_correct\"] = (df_zs[\"true_label\"] == df_zs[\"pred_label\"])\n",
    "save_errors(df_zs, \"zero_shot\")\n",
    "\n",
    "# also store metrics quick\n",
    "print(\"Zero-shot metrics:\",\n",
    "      {\"acc\": accuracy_score(y_true, y_pred_zs),\n",
    "       \"macro_f1\": f1_score(y_true, y_pred_zs, average=\"macro\"),\n",
    "       \"weighted_f1\": f1_score(y_true, y_pred_zs, average=\"weighted\")})\n",
    "\n",
    "# ================= 2) Full fine-tuned model =================\n",
    "print(\"\\n[2/3] Full FT predictions (roberta-large-mnli fine-tuned)…\")\n",
    "tok_full = AutoTokenizer.from_pretrained(FULL_DIR)\n",
    "model_full = AutoModelForSequenceClassification.from_pretrained(FULL_DIR).to(\"cuda\" if device == 0 else \"cpu\")\n",
    "def predict_prob(model, tokenizer, texts, max_len=MAX_LEN, batch=BATCH):\n",
    "    probs_list = []\n",
    "    model.eval()\n",
    "    for i in range(0, len(texts), batch):\n",
    "        chunk = texts[i:i+batch]\n",
    "        enc = tokenizer(chunk, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_len)\n",
    "        enc = {k: v.to(model.device) for k, v in enc.items()}\n",
    "        with torch.no_grad():\n",
    "            logits = model(**enc).logits\n",
    "            probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "        probs_list.append(probs)\n",
    "    return np.vstack(probs_list)\n",
    "\n",
    "probs_full = predict_prob(model_full, tok_full, texts)\n",
    "y_pred_full = probs_full.argmax(axis=-1)\n",
    "conf_full = probs_full.max(axis=-1)\n",
    "\n",
    "df_full = pd.DataFrame({\n",
    "    \"text\": df[\"text\"],\n",
    "    \"true_label\": df[\"label\"],\n",
    "    \"pred_label\": [id2label[i] for i in y_pred_full],\n",
    "    \"confidence\": conf_full,\n",
    "})\n",
    "df_full[\"is_correct\"] = (df_full[\"true_label\"] == df_full[\"pred_label\"])\n",
    "save_errors(df_full, \"full\")\n",
    "\n",
    "print(\"Full-FT metrics:\",\n",
    "      {\"acc\": accuracy_score(y_true, y_pred_full),\n",
    "       \"macro_f1\": f1_score(y_true, y_pred_full, average=\"macro\"),\n",
    "       \"weighted_f1\": f1_score(y_true, y_pred_full, average=\"weighted\")})\n",
    "\n",
    "# ================= 3) LoRA fine-tuned model =================\n",
    "print(\"\\n[3/3] LoRA FT predictions (roberta-large-mnli + adapters)…\")\n",
    "tok_lora = AutoTokenizer.from_pretrained(BASELINE_NAME)\n",
    "base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    BASELINE_NAME, num_labels=num_labels, ignore_mismatched_sizes=True\n",
    ")\n",
    "model_lora = PeftModel.from_pretrained(base, LORA_DIR).to(\"cuda\" if device == 0 else \"cpu\")\n",
    "\n",
    "probs_lora = predict_prob(model_lora, tok_lora, texts)\n",
    "y_pred_lora = probs_lora.argmax(axis=-1)\n",
    "conf_lora = probs_lora.max(axis=-1)\n",
    "\n",
    "df_lora = pd.DataFrame({\n",
    "    \"text\": df[\"text\"],\n",
    "    \"true_label\": df[\"label\"],\n",
    "    \"pred_label\": [id2label[i] for i in y_pred_lora],\n",
    "    \"confidence\": conf_lora,\n",
    "})\n",
    "df_lora[\"is_correct\"] = (df_lora[\"true_label\"] == df_lora[\"pred_label\"])\n",
    "save_errors(df_lora, \"lora\")\n",
    "\n",
    "print(\"LoRA-FT metrics:\",\n",
    "      {\"acc\": accuracy_score(y_true, y_pred_lora),\n",
    "       \"macro_f1\": f1_score(y_true, y_pred_lora, average=\"macro\"),\n",
    "       \"weighted_f1\": f1_score(y_true, y_pred_lora, average=\"weighted\")})\n",
    "\n",
    "# ---------------- Merged comparison & disagreements ----------------\n",
    "all_preds = pd.DataFrame({\n",
    "    \"text\": df[\"text\"],\n",
    "    \"true\": df[\"label\"],\n",
    "    \"pred_zero_shot\": [id2label[i] for i in y_pred_zs],\n",
    "    \"conf_zero_shot\": conf_zs,\n",
    "    \"pred_full\": [id2label[i] for i in y_pred_full],\n",
    "    \"conf_full\": conf_full,\n",
    "    \"pred_lora\": [id2label[i] for i in y_pred_lora],\n",
    "    \"conf_lora\": conf_lora,\n",
    "})\n",
    "all_path = OUT_DIR / \"all_preds.csv\"\n",
    "all_preds.to_csv(all_path, index=False)\n",
    "print(f\"Saved: {all_path}\")\n",
    "\n",
    "disagreements = all_preds[\n",
    "    (all_preds[\"pred_zero_shot\"] != all_preds[\"true\"]) |\n",
    "    (all_preds[\"pred_full\"] != all_preds[\"true\"]) |\n",
    "    (all_preds[\"pred_lora\"] != all_preds[\"true\"])\n",
    "].copy()\n",
    "disagreements_path = OUT_DIR / \"disagreements.csv\"\n",
    "disagreements.to_csv(disagreements_path, index=False)\n",
    "print(f\"Saved: {disagreements_path}\")\n",
    "\n",
    "# ---------------- Per-model confusion pairs & hardest examples ----------------\n",
    "# Zero-shot\n",
    "top_confusions(y_true, y_pred_zs, id2label, k=10, fname=\"top_confusions_zero_shot.csv\")\n",
    "hardest_examples(df, y_true, y_pred_zs, conf_zs, id2label, per_class=5, fname=\"hardest_zero_shot.csv\")\n",
    "\n",
    "# Full FT\n",
    "top_confusions(y_true, y_pred_full, id2label, k=10, fname=\"top_confusions_full.csv\")\n",
    "hardest_examples(df, y_true, y_pred_full, conf_full, id2label, per_class=5, fname=\"hardest_full.csv\")\n",
    "\n",
    "# LoRA FT\n",
    "top_confusions(y_true, y_pred_lora, id2label, k=10, fname=\"top_confusions_lora.csv\")\n",
    "hardest_examples(df, y_true, y_pred_lora, conf_lora, id2label, per_class=5, fname=\"hardest_lora.csv\")\n",
    "\n",
    "print(\"\\nDone. Check the 'error_analysis/' folder for CSV outputs.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
